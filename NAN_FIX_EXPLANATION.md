# NaN Embedding Issue - Fix Summary

## Problem Description

When processing document entities with the LightRAG system, embeddings generated by Ollama could contain NaN (Not-a-Number) values. These NaN values caused the application to fail with the following error:

```
Error processing entity `Streaming Rights`: VDB entity_upsert failed for Streaming Rights after 3 attempts: 
failed to encode response: json: unsupported value: NaN (status code: 500)
```

### Root Cause

The error occurred in the following sequence:

1. **Embedding Generation**: When `ollama_embed()` in `lightrag/llm/ollama.py` calls the Ollama API to generate embeddings, the response could contain NaN values (likely due to edge cases in the Ollama model's output or incomplete responses).

2. **Vector Storage**: The embeddings are stored in PostgreSQL using the pgvector extension (a vector type for PostgreSQL). pgvector, like PostgreSQL's native JSON type, doesn't support NaN values for serialization.

3. **JSON Encoding Failure**: When the system attempts to store or serialize these embeddings for the vector database, the JSON encoder fails because NaN values are not valid JSON according to the JSON specification.

### Error Stack Trace Analysis

- **Location**: `lightrag/kg/postgres_impl.py`, line 2961-2965 in the `VectorStorageImpl.upsert()` method
- **Key Step**: After `embeddings_list = await asyncio.gather(*embedding_tasks)`, the embeddings are concatenated and assigned to data items
- **Issue**: If embeddings contain NaN, they fail when pgvector tries to encode them

## Solution

The fix adds **NaN sanitization** immediately after embeddings are gathered and concatenated, using NumPy's built-in `nan_to_num()` function.

### Implementation

In `/mnt/beckie2/LIGHT.RAG/LightRAG/lightrag/kg/postgres_impl.py` (around line 2963-2964):

```python
embeddings = np.concatenate(embeddings_list)
# Sanitize embeddings: replace NaN values with 0.0 to prevent JSON serialization errors
embeddings = np.nan_to_num(embeddings, nan=0.0, posinf=1.0, neginf=-1.0)
for i, d in enumerate(list_data):
    d["__vector__"] = embeddings[i]
```

### What the Fix Does

The `np.nan_to_num()` function:
- **NaN values** → Replaced with `0.0`
- **Positive infinity** → Replaced with `1.0`
- **Negative infinity** → Replaced with `-1.0`
- **Other values** → Left unchanged

This ensures that:
1. All embeddings are valid floating-point numbers
2. The vectors can be properly serialized and stored in pgvector
3. The system continues processing without errors
4. The embeddings remain semantically meaningful (zero is a neutral value in embedding space)

## Testing

To verify the fix works:

1. Run the application with the problematic entity name or document that previously caused NaN errors
2. Monitor the logs to ensure entities are successfully processed
3. Verify that vector searches work correctly with the sanitized embeddings

## Alternative Approaches Considered

1. **Replacing NaN with mean/median**: More complex, no significant benefit
2. **Filtering out NaN embeddings**: Loses data
3. **Fixing the Ollama output**: Would require changes to the embedding model itself
4. **Converting to float32**: Doesn't solve the NaN problem

The chosen approach (NaN → 0.0) is:
- Simple and reliable
- Maintains vector count and structure
- Doesn't introduce unnecessary complexity
- Handles edge cases gracefully

## Files Modified

- `lightrag/kg/postgres_impl.py`: Added NaN sanitization in `VectorStorageImpl.upsert()` method (line ~2964)

## Notes

- This is a defensive fix that handles unexpected NaN values from any embedding provider
- The fix is applied at the database layer, so it works regardless of where NaN values originate
- No changes to the API or user-facing functionality
